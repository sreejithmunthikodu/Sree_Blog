---
title: My blog on Descriptive Statistics
tags:
- Machine Learning
- Gradient Descent
desc: Basic Machine Learning
layout: post
--- 

Ever wondered how kids learn as they grow? They continuously learn from their experiences. For example, they learn to distinguish between a cat and a dog after they see many cats and dogs. Similarly, we can teach computers to learn from experiences, which are data. Traditionally, computers followed a set of instructions called programs to do a task. However, this approach is not effective in complex tasks such as image recognition and product recommendation, due to complex rules to write the function. In machine learning, we give computers input and output data and train them to figure out the best program by themselves to perform tasks without explicitly coding. This enables computers to do complex tasks which are not possible by conventional programming methods.

More formally, machine learning is the process of training computers to learn from data to do tasks without explicitly programming. Training is the process by which computers learn from the data and training data are the data used for training computers. Voice recognition, image recognition, and movie recommendation are all examples of machine learning tasks. 

Consider an image recognition machine learning model. The task of a machine learning model here is to correctly classify given images into one of the many categories. Machine learning models achieve this by learning from the training data. The training data consist of many images labeled as per their correct category. We assume that there exists a function that can accurately map given input images to their respective output labels. Machine learning lets the algorithm approximate this function from the data. We start the training by approximating a generalized expression of the function with many tunable parameters. For example, we can write a simple linear approximation of an image recognition algorithm as follows,

$W_1X_1 + W_2X_2 + W_3X_3 + ......+ W_nX_n= 1 $ if image is from category 1, $0$ otherwise

Here, $X_1$, $X_2$, $X_3$ ... $X_n$ are called the features of the data. Features are the values extracted from the input data on which we can do mathematical operations. A simple set of features for the above image recognition task would be the pixel values of the image.
$W_1$, $W_2$, ... $W_n$ are called the parameters or the weights of the machine learning model. If we change the parameters, we get different functions. In fact, there are infinite functions possible from the above general expression. Training the machine learning model means tuning these parameters such that the final equation represents the best possible form of the above expression that accurately maps the input to their respective output. If we consider input images of size (320px, 320px, 3), there are 307200 parameters to tune while training. The above linear model is very simplified and less effective in image recognition tasks. More complex expressions would perform better and would have millions of parameters to tune.

So far, we learned that training a machine model is actually finding the best values of the parameters so that it accurately predicts the output. We begin the training with randomly initialized parameters. Then the algorithm uses this random model to make predictions on all the images in the training dataset. The performance will be poor as the model is not tuned. Algorithm then evaluates the total error in prediction on the entire training images. This error is called the training loss. Our goal is to reduce training loss by changing the parameters, $W_1$, $W_2$,... $W_n$. We achieve this by a process called optimization. If we have only two parameters, $W_1$ and $W_2$, we can visualize the plot of loss, which is a function of the two parameters, as shown below. The goal of machine learning model is to find the values of the parameters $W_1$ and $W_2$ so that the loss is the minimum. So, we can imagine as if we need to maneuver a valley spanned by the parameters and to reach the bottom, which has the lowest loss. If we extend this to $n$ number of parameters, we are looking at finding the smallest value of loss function in an $n$ dimensional hyperplane spanned by the $n$ parameters. Remember, $n$  could be in millions for complex tasks such as image recognition using advanced machine learning algorithms.

More formally, machine learning is the process of training computers to learn from data to do tasks without explicitly programming. Training is the process by which computers learn from the data and training data are the data used for training computers. Voice recognition, image recognition, and movie recommendation are all examples of machine learning tasks. 

Consider an image recognition machine learning model. The task of a machine learning model here is to correctly classify given images into one of the many categories. Machine learning models achieve this by learning from the training data. The training data consist of many images labeled as per their correct category. We assume that there exists a function that can accurately map given input images to their respective output labels. Machine learning lets the algorithm approximate this function from the data. We start the training by approximating a generalized expression of the function with many tunable parameters. For example, we can write a simple linear approximation of an image recognition algorithm as follows,

$W_1X_1 + W_2X_2 + W_3X_3 + ......+ W_nX_n= 1 $ if image is from category 1, $0$ otherwise

Here, $X_1$, $X_2$, $X_3$ ... $X_n$ are called the features of the data. Features are the values extracted from the input data on which we can do mathematical operations. A simple set of features for the above image recognition task would be the pixel values of the image.
$W_1$, $W_2$, ... $W_n$ are called the parameters or the weights of the machine learning model. If we change the parameters, we get different functions. In fact, there are infinite functions possible from the above general expression. Training the machine learning model means tuning these parameters such that the final equation represents the best possible form of the above expression that accurately maps the input to their respective output. If we consider input images of size (320px, 320px, 3), there are 307200 parameters to tune while training. The above linear model is very simplified and less effective in image recognition tasks. More complex expressions would perform better and would have millions of parameters to tune.

So far, we learned that training a machine model is actually finding the best values of the parameters so that it accurately predicts the output. We begin the training with randomly initialized parameters. Then the algorithm uses this random model to make predictions on all the images in the training dataset. The performance will be poor as the model is not tuned. Algorithm then evaluates the total error in prediction on the entire training images. This error is called the training loss. Our goal is to reduce training loss by changing the parameters, $W_1$, $W_2$,... $W_n$. We achieve this by a process called optimization. If we have only two parameters, $W_1$ and $W_2$, we can visualize the plot of loss, which is a function of the two parameters, as shown below. The goal of machine learning model is to find the values of the parameters $W_1$ and $W_2$ so that the loss is the minimum. So, we can imagine as if we need to maneuver a valley spanned by the parameters and to reach the bottom, which has the lowest loss. If we extend this to $n$ number of parameters, we are looking at finding the smallest value of loss function in an $n$ dimensional hyperplane spanned by the $n$ parameters. Remember, $n$  could be in millions for complex tasks such as image recognition using advanced machine learning algorithms.

More formally, machine learning is the process of training computers to learn from data to do tasks without explicitly programming. Training is the process by which computers learn from the data and training data are the data used for training computers. Voice recognition, image recognition, and movie recommendation are all examples of machine learning tasks. 

Consider an image recognition machine learning model. The task of a machine learning model here is to correctly classify given images into one of the many categories. Machine learning models achieve this by learning from the training data. The training data consist of many images labeled as per their correct category. We assume that there exists a function that can accurately map given input images to their respective output labels. Machine learning lets the algorithm approximate this function from the data. We start the training by approximating a generalized expression of the function with many tunable parameters. For example, we can write a simple linear approximation of an image recognition algorithm as follows,

$W_1X_1 + W_2X_2 + W_3X_3 + ......+ W_nX_n= 1 $ if image is from category 1, $0$ otherwise

Here, $X_1$, $X_2$, $X_3$ ... $X_n$ are called the features of the data. Features are the values extracted from the input data on which we can do mathematical operations. A simple set of features for the above image recognition task would be the pixel values of the image.
$W_1$, $W_2$, ... $W_n$ are called the parameters or the weights of the machine learning model. If we change the parameters, we get different functions. In fact, there are infinite functions possible from the above general expression. Training the machine learning model means tuning these parameters such that the final equation represents the best possible form of the above expression that accurately maps the input to their respective output. If we consider input images of size (320px, 320px, 3), there are 307200 parameters to tune while training. The above linear model is very simplified and less effective in image recognition tasks. More complex expressions would perform better and would have millions of parameters to tune.

So far, we learned that training a machine model is actually finding the best values of the parameters so that it accurately predicts the output. We begin the training with randomly initialized parameters. Then the algorithm uses this random model to make predictions on all the images in the training dataset. The performance will be poor as the model is not tuned. Algorithm then evaluates the total error in prediction on the entire training images. This error is called the training loss. Our goal is to reduce training loss by changing the parameters, $W_1$, $W_2$,... $W_n$. We achieve this by a process called optimization. If we have only two parameters, $W_1$ and $W_2$, we can visualize the plot of loss, which is a function of the two parameters, as shown below. The goal of machine learning model is to find the values of the parameters $W_1$ and $W_2$ so that the loss is the minimum. So, we can imagine as if we need to maneuver a valley spanned by the parameters and to reach the bottom, which has the lowest loss. If we extend this to $n$ number of parameters, we are looking at finding the smallest value of loss function in an $n$ dimensional hyperplane spanned by the $n$ parameters. Remember, $n$  could be in millions for complex tasks such as image recognition using advanced machine learning algorithms.
